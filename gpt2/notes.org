* Links

- [[https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8][Let's build GPT: from scratch, in code, spelled out.]]: Lecture we are currently
  watching.
- [[https://github.com/karpathy/nanochat][GH nanochat]]: "This repo is a full-stack implementation of an LLM like ChatGPT
  in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is
  designed to run on a single 8XH100 node via scripts like speedrun.sh, that run
  the entire pipeline start to end. This includes tokenization, pretraining,
  finetuning, evaluation, inference, and web serving over a simple UI so that
  you can talk to your own LLM just like ChatGPT."

- [[https://github.com/openai/tiktoken][GH: tiktoken]]: "tiktoken is a fast [[https://en.wikipedia.org/wiki/Byte-pair_encoding][BPE]] tokeniser for use with OpenAI's models."
- [[https://github.com/google/sentencepiece][GH: sentencepiece]]: "SentencePiece is an unsupervised text tokenizer and
  detokenizer mainly for Neural Network-based text generation systems where the
  vocabulary size is predetermined prior to the neural model training.
  SentencePiece implements subword units (e.g., byte-pair-encoding (BPE)
  [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of
  direct training from raw sentences. SentencePiece allows us to make a purely
  end-to-end system that does not depend on language-specific
  pre/postprocessing."
- [[https://github.com/karpathy/ng-video-lecture/tree/master][GH: ng-video-lecture:]] "Code created in the Neural Networks: Zero To Hero video
  lecture series, specifically on the first lecture on nanoGPT. Publishing here
  as a Github repo so people can easily hack it, walk through the git log
  history of it, etc."

* BPE

#+begin_quote
In computing, byte-pair encoding (BPE), or digram coding, is an algorithm, first
described in 1994 by Philip Gage, for encoding strings of text into smaller
strings by creating and using a translation table. A slightly modified version
of the algorithm is used in large language model tokenizers.

The original version of the algorithm focused on compression. It replaces the
highest-frequency pair of bytes with a new byte that was not contained in the
initial dataset. A lookup table of the replacements is required to rebuild the
initial dataset. The modified version builds "tokens" (units of recognition)
that match varying amounts of source text, from single characters (including
single digits or single punctuation marks) to whole words (even long compound
words).
#+end_quote

* Why does Karpathy use random locations

Karpathy uses *random locations* for sampling training batches in his GPT model
implementations (like minGPT/nanoGPT) for several key reasons related to
*generalization* and *computational efficiency* of the training process.

The line of code in the image:

#+begin_src python
ix = torch.randint(len(data) - block_size, (batch_size,))
#+end_src

This command randomly selects =batch_size= number of starting indices (=ix=)
from the dataset (=data=). Each index represents the start of a sequence of
length =block_size= that will form an individual input/target pair in the batch.

** Key Reasons for Random Sampling

*** Generalisation and Reducing Data Correlation (Shuffling)

- *Avoid Sequential Bias:* If the sequences were taken in a purely sequential
  order (e.g., from index 0 to $B$, then $B$ to $2B$, etc.), the model could
  learn patterns specific to the contiguous chunks of text. This is often poor
  practice because it means the model is always seeing one specific context
  after another.
- *Representative Batches:* Randomly sampling ensures that each batch is a
  *mini-representation* of the entire dataset. This is crucial for the
  stochastic gradient descent (SGD) optimization algorithm. A random batch
  provides an unbiased estimate of the overall training loss gradient, which
  helps the model **generalize** better to unseen data and prevents it from
  over-fitting to specific, local sequences in the data.

*** Utilising All Data Points

- *Full Context Coverage:* When training language models on large sequential
  data (like a long text file), there are many overlapping possible
  sub-sequences of the fixed length (`block_size`). Random sampling over the
  entire dataset allows **every character/token** to be the starting point for a
  sequence at some point during training. This maximises the utilization of the
  training data.

*** Stability and Noise for Optimisation

- *Noise in the Gradient:* Using random, smaller batches (mini-batch SGD)
  introduces a beneficial **stochasticity** (noise) into the gradient updates.
  This noise helps the model escape sharp local minima in the loss landscape and
  often leads to convergence in "flatter" minima, which are associated with
  better *generalization* performance. If the batch contained the entire dataset
  (full-batch GD), the update would be exact but could get stuck in a poor
  minimum.

** Summary

In essence, the use of =torch.randint= is a standard and effective data loading
technique to *shuffle* the data at the batch level, optimising for faster and
more stable training convergence with better generalisation.

* What are the Logits?

In this context, logits are the raw, unnormalized prediction scores that the
model outputs for every possible next token in the vocabulary.

** The Output Tensor

The line =logits = self.token_embedding_table(idx)= produces a tensor with the
shape =(B, T, C)=:

- *B (Batch Size)*: The number of independent sequences being processed
  simultaneously (/e.g./, 4).
- *T (Time/Block Length)*: The context length or sequence length (/e.g./, 8).
- *C (Vocabulary Size)*: The total number of unique tokens / characters in your
  dataset.

For every single input token in your sequence, the model outputs a vector of
length =C=, where each value is a score predicting how likely the corresponding
token in the vocabulary is to be the next token.

** The Bigram Mechanism

In this specific Bigram model:

- The =nn.Embedding= layer acts as both the input embedding and the output layer
  (logits layer).
- The embedding table has the shape =(vocab_size, vocab_size)=.

When you pass an input index =idx= (a token index) into this table, it looks up
the row corresponding to that token.

That retrieved row is the vector of raw scores (logits) for the next possible
token. Essentially, the model is learning the simple probability of any token C
following the current token =i= directly from a lookup table.

** How Logits are Used

*** Training (The forward function)

The logits are converted into a loss value:

- *Reshaping*: The =(B, T, C)= logits tensor and the =(B, T)= targets tensor are
  flattened into =(B*T, C)= and =(B*T)= respectively. This pools all independent
  predictions across the batch and time steps.

- *Cross Entropy*: The function =F.cross_entropy(logits, targets)= is applied.
  This function does two things internally:

  - It applies the Softmax function to the logits to convert them into
    probabilities (where all probabilities sum to 1).
  - It calculates the negative log-likelihood loss between these probabilities
    and the true targets. The model uses this loss to adjust its weights (the
    embedding table) via backpropagation.

*** Inference/Generation (The generate function)

The logits are converted into a next token prediction:

Focus on *Last Token*: =logits = logits[:, -1, :]= selects only the prediction
for the very last token in the context, as that's the only one relevant for
predicting the new token.

- *Softmax*: =probs = F.softmax(logits, dim=-1)= converts the raw scores
  (logits) into a proper probability distribution.
- *Sampling*: =idx_next = torch.multinomial(probs, num_samples=1)= samples the
  next token index from this probability distribution, introducing an element of
  randomness (creativity) into the generation.
